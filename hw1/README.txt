Q1: Curation Rationale Which texts were included and what were the goals in selecting texts?
Ans: The Sentiment Treebank was curated to caputure a balanced range of sentiments in movie reviews. From my understanding, the texts were chosen because movie reviews naturally contain good sentiment cues i.e. from subjective opinions to evaluative adjectives that are useful for training sentiment classification models. The selection aims to ensure sufficient lexical variety and a realistic distribution of tokens(as seen in our vocabulary outputs, where high-frequency function words co-occur with sentiment-bearing adjectives) while filtering out very rare words using a minimum frequency threshold to reduce noise and sparsity in model input.

Q2: Language Variety From what language variety are the texts? You may provide a prose description
and/or a BCP47 code.
Ans: The texts are written in informal American English, characteristic of everyday online communication and movie reviews. They exhibit colloquial language, idiomatic expressions, and occasional non-standard grammatical constructions typical of user-generated content. In BCP 47 language tag terms, this variety can be designated as en-US. 

Q.3: Speaker Demographics Who were the producers of the texts? Information may include (if available): age, gender, native language, socioeconomic status, number of speakers.
Ans: The movie reviews were written by everyday moviegoers who share their opinions online. While the dataset does not include explicit demographic metadata (such as age, gender, or socioeconomic status), the language used and the style of writing suggest a diverse group of speakers. In NLP, such diversity is crucial for building robust sentiment models; however, the lack of detailed demographic labels means that potential biases may be present and are hard to quantify.

Q4: Annotator Demographics Who were the annotators of the data? Information may include (if available): age, gender, native language, socioeconomic status, number of annotators.
Ans: The sentiment annotations in the dataset were likely performed using crowd-sourcing techniques (e.g., through platforms like Amazon Mechanical Turk) where annotators provided fine-grained sentiment labels for phrases or sentences. Although explicit demographic information about the annotators (such as age, gender, native language) is not provided, the assumption is that they are native or fluent speakers of English. From an NLP research perspective, the absence of detailed annotator demographics makes it difficult to assess inter-annotator agreement biases or cultural influences on sentiment judgments.

Q5: Speech Situation What were the conditions of text production? Details may include whether it was written or spoken, whether it was spontaneous or not, and who the intended audience was.
Ans: The texts were produced as written, spontaneous online movie reviews rather than scripted or spoken language. Reviewers typically typed their immediate reactions with little post-editing, resulting in informal, colloquial language that reflects genuine emotional responses. The intended audience is the general public—especially potential movie-goers—so the language is natural, unstructured, and often includes irregular punctuation and non-standard grammar, all of which present unique challenges for NLP tasks such as sentiment analysis and text normalization.

Q6: Text Characteristics What are the genre and topic of the texts?
Ans: The texts belong to the genre of user-generated movie reviews, and their primary topic is film evaluation. They consist of subjective, evaluative language where reviewers express opinions, critique film quality, and share personal experiences related to movies. This genre is characterized by informal language, varied sentence structures, and the use of colloquial expressions, which together present both a rich resource for sentiment analysis and challenges for tasks like syntactic parsing and semantic interpretation.

Q7: Reflections Which of these questions were hard to answer with the information provided about the
dataset? Why might it be helpful to have that information more explicitly documented?
Ans:One of the most challenging questions to answer based on the available documentation is the detailed demographic information for both the text producers and the annotators (Q3 and Q4). The dataset documentation on the Stanford Sentiment Treebank (as seen on the website and in the paper) provides little to no information about the age, gender, socioeconomic status, or native language of either the reviewers or the annotators.
Having this information more explicitly documented would be valuable for several reasons:
Bias Assessment: It would enable researchers to examine whether the language and sentiment annotations are influenced by demographic factors. This could help in understanding and mitigating potential biases in the resulting NLP models.
Model Generalizability: Knowing the demographic distribution of the speakers and annotators can assist in evaluating the extent to which models trained on this data can generalize to broader or different populations.
Interpretability and Fairness: Detailed metadata would provide context for interpreting model performance, especially in fairness-sensitive applications, by making it clear how well the data represents the target user population.
Improved Data Statements: In line with the ideas presented in the data statement literature (e.g., Bender and Friedman’s work), explicit documentation fosters transparency, reproducibility, and accountability
